{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "99f089a5",
      "metadata": {
        "id": "99f089a5"
      },
      "source": [
        "# Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60da0557",
      "metadata": {
        "id": "60da0557"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SQLContext\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace97898",
      "metadata": {
        "id": "ace97898"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94080c5e",
      "metadata": {
        "id": "94080c5e"
      },
      "outputs": [],
      "source": [
        "class SparkClass:\n",
        "    def __init__(self):\n",
        "        self.spark = self.__init_spark()\n",
        "\n",
        "    def __init_spark(self):\n",
        "        return SparkSession.builder.appName(\"MyProcess\").master(\"local[*]\").getOrCreate()\n",
        "\n",
        "    def read_csv(self, path):\n",
        "        try:\n",
        "            df = self.spark.read.option(\"header\", True).csv(path, sep=';')\n",
        "        except Exception as err:\n",
        "            raise Exception(err)\n",
        "        return df\n",
        "\n",
        "    def read_xlsx(self, path):\n",
        "        try:\n",
        "            df = pd.read_excel(path)\n",
        "        except Exception as err:\n",
        "            raise Exception(err)\n",
        "        return df\n",
        "\n",
        "    #ler como string\n",
        "    def read_xml(self, path):\n",
        "        try:\n",
        "            df = self.spark.read.text(path, wholetext=False)\n",
        "            #df = self.spark.read.text(path, wholetext=False).rdd ler com RDD\n",
        "        except Exception as err:\n",
        "            raise Exception(err)\n",
        "        return df\n",
        "\n",
        "    def readExcel(self , path):\n",
        "        try:\n",
        "            df = self.spark.read.format('com.crealytics.spark.excel').option(\"location\", file_location).option(\"header\", \"true\").option(\"addColorColumns\", \"false\").option(\"treatEmptyValuesAsNulls\", \"false\").option(\"inferSchema\", \"true\").load(file_location)\n",
        "        except Exception as err:\n",
        "            raise Exception(err)\n",
        "        return df\n",
        "\n",
        "    def csv_to_parquet(self, path_input, path_output):\n",
        "        try:\n",
        "            (\n",
        "                spark.read.format(\"com.crealytics.spark.excel\").load(path_input, header=True)\n",
        "                .write.format('parquet').save(path_output)\n",
        "            )\n",
        "        except Exception as err:\n",
        "            raise Exception(err)\n",
        "        return df\n",
        "\n",
        "    def sum_columns(self, df, column_name, columns_list):\n",
        "        try:\n",
        "            r = df.withColumn(column_name, sum(columns_list))\n",
        "        except Exception as err:\n",
        "            raise Exception(err)\n",
        "\n",
        "        return r.select(column_name)\n",
        "\n",
        "    def sub_columns(self, df, column_name, columns_list):\n",
        "        try:\n",
        "            r = df.withColumn(column_name, columns_list[0] - columns_list[1])\n",
        "        except Exception as err:\n",
        "            raise Exception(err)\n",
        "\n",
        "        return r.select(column_name)\n",
        "\n",
        "\n",
        "    def date_dif(self, df, column_name, colHighDate, colLowDate):\n",
        "        try:\n",
        "            r =  df.withColumn(column_name, datediff(colHighDate, colLowDate))\n",
        "        except Exception as err:\n",
        "            raise Exception(err)\n",
        "\n",
        "        return r.select(column_name)\n",
        "\n",
        "    def sort_column(self, df, column, sort_type):\n",
        "        valid_sort_types = [\"asc\", \"desc\"]\n",
        "\n",
        "        if sort_type not in valid_sort_types:\n",
        "            print(\"Invalid sort type\")\n",
        "\n",
        "        if sort_type == \"asc\":\n",
        "            r = df.sort(column.asc())\n",
        "        else:\n",
        "            r = df.sort(column.desc())\n",
        "\n",
        "        return r\n",
        "\n",
        "    def rate_column(self, df, column_name, column, double = False): #double é para situações de periodos duplos\n",
        "        try:\n",
        "            if double:\n",
        "                r = df.withColumn(column_name, when(column == 0, \"AA\")\n",
        "                                        .when(column < 29, \"A\")\n",
        "                                        .when(column < 59, \"B\")\n",
        "                                        .when(column < 119, \"C\")\n",
        "                                        .when(column < 179, \"D\")\n",
        "                                        .when(column < 239, \"E\")\n",
        "                                        .when(column < 299, \"F\")\n",
        "                                        .when(column < 359, \"G\")\n",
        "                                        .otherwise(\"H\"))\n",
        "            else:\n",
        "                    r = df.withColumn(column_name, when(column == 0, \"AA\")\n",
        "                                        .when(column < 15, \"A\")\n",
        "                                        .when(column < 31, \"B\")\n",
        "                                        .when(column < 61, \"C\")\n",
        "                                        .when(column < 91, \"D\")\n",
        "                                        .when(column < 121, \"E\")\n",
        "                                        .when(column < 151, \"F\")\n",
        "                                        .when(column < 180, \"G\")\n",
        "                                        .otherwise(\"H\"))\n",
        "        except Exception as err:\n",
        "            raise Exception(err)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e832a79",
      "metadata": {
        "id": "1e832a79",
        "outputId": "f80e0d02-8718-4114-ae05-a232730583db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame[COL1: string, COL2: string]\n"
          ]
        }
      ],
      "source": [
        "path = r\"input\\TESTE.csv\"\n",
        "\n",
        "s = SparkClass()\n",
        "df = s.read_csv(path)\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0516edac",
      "metadata": {
        "id": "0516edac"
      },
      "source": [
        "https://ssopenpyxl.readthedocs.io/en/2.5.2/openpyxl.formula.translate.html\n",
        "    replicar formulas no excel, com referencia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40a9d28c",
      "metadata": {
        "id": "40a9d28c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
